Random variables are underlying values that are determined by the underlying random experiment. Random variable can be
thought of as a single instance of a random experiment that we want to find a probability estimate for. It is a function
from the sample space to the real valued numbers, in that this variable tries to quantify what real value a random
variable will get.

There are two types of random variables, discrete and continuous.

For every discrete random variable, we have a range defined, which is the mapping to the all numbers except rational.
The range can thus be 0, 1, 2 if we choose to flip a coin twice.

For a random variable, the probabilities of events {X = xk} are shown by the Probability Mass Function. The PMF is
usually defined over the range of a random variable.

We can use PMF to find the probability distribution of a discrete random variable. PMF itself is a probability measure,
and is thus always between 0 and 1.

The notion of independent events can be extended to independent random variables.

There are a set of distributions that are frequently used in computation of the PMF for discrete events, and they are as
follows:

1) Bernoulli Distribution: This distribution is defined for discrete random variables that can only take two values,
0 and 1 indicating success or failure. It is also called an indicator random variable, as it indicates whether
an event will happen (kinda like a signal in stock market). The experiment associated with this is the coin toss.

2) Geometric Distribution: This distribution is defined for a random experiment is repeated until the desired value of
the random variable is achieved. The experiment associated with this type of distribution is when flipping a coin
until a head or a tail is observed.

3) Binomial Distribution: The experiment associated with this type of distribution is when we keep flipping a coin
and want to get the probability of seeing a head n times. This is given by nCk, a combinatorial counting approach.

The PMF function cannot be used to define continuous random variables. For that, we have the Cumulative Distribution
Function, CDF, and this works for all types of random variables.

The CDF is defined on the lower bound of random variables, and is a step function while the PMF is always on the
x-axis. The CDF is always a non-decreasing function.

The average value of a random variable is defined as the `expectation` of the random variable. Expected value is
defined as the weighted average of the range of a random variable. The intuition behind the expected value is that
a random experiment is conducted N number of times, each experiment being independent, and the average value of the
random variable for which the experiment is conducted N times is then called the expectation of the random variable.
The formula for computing the expectation of a random variable is given by the range(i)*P(X == range(i)). This holds
true across all the different probability distributions.

The variance of a random variable can be defined as how far away from the expected value are the probability events.
If the individual probability events are far away from the expected value of the random variable, then we say that the
variance is high, or vice versa.

The variance of a random variable X can be given by E(X^2) - (EX)^2, i.e., we compute the normal expectation first,
and then we compute the expectation using the square of the events of the random variable.

Variance of a random variable is not a linear operation, since we are subtracting the square of the random variable
with the normal random variable expectation.



